import serial
import csv
import time
from datetime import datetime
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import StandardScaler
import tensorflow.keras.backend as K

# working directory should be NeverLateX
# run command: sudo python3 /Users/tunakisaga/Documents/GitHub/NeverLateX/all_sensors/all_sensors_only_prediction.py

# === Functions ===
def report_predicted_characters(predicted_characters):
    print("üî† Predicted Characters: üî†")
    for timestamp, letter in predicted_characters.items():
        print(f"{timestamp}: {letter}")
        
def ctc_loss_fn(y_true, y_pred):
    # Cast labels to int32
    y_true = tf.cast(y_true, dtype=tf.int32)

    # Input length: Number of time steps for each input
    input_length = tf.fill([tf.shape(y_pred)[0]], tf.shape(y_pred)[1])

    # Label length: Actual length of each label
    label_length = tf.reduce_sum(tf.cast(tf.not_equal(y_true, 0),
                                         dtype=tf.int32), axis=1)

    # Compute CTC loss
    return tf.reduce_mean(
        tf.nn.ctc_loss(
            labels=y_true,
            logits=y_pred,
            label_length=label_length,
            logit_length=input_length,
            logits_time_major=False,
            blank_index=-1,  # Use the last class as the blank label
        )
    )
          
class CTCAccuracy(tf.keras.metrics.Metric):
    def __init__(self, name="ctc_accuracy", **kwargs):
        super(CTCAccuracy, self).__init__(name=name, **kwargs)
        self.correct_predictions = self.add_weight(name="correct", initializer="zeros", dtype=tf.float32)
        self.total_samples = self.add_weight(name="total", initializer="zeros", dtype=tf.float32)

    def update_state(self, y_true, y_pred, sample_weight=None):
        # Decode predictions using greedy search
        y_pred_decoded, _ = K.ctc_decode(y_pred,
                                         input_length=tf.fill([tf.shape(y_pred)[0]], tf.shape(y_pred)[1]),
                                         greedy=True)

        # Ensure we are working with a sparse tensor before converting to dense
        if isinstance(y_pred_decoded[0], tf.SparseTensor):
            y_pred_dense = tf.sparse.to_dense(y_pred_decoded[0], default_value=-1)
        else:
            y_pred_dense = y_pred_decoded[0]  # If already dense, use it directly

        # Convert ground truth labels to int32
        y_true_dense = tf.cast(y_true, dtype=tf.int32)
        y_pred_dense = tf.cast(y_pred_dense, dtype=tf.int32)  # Ensure both are int32

        # Compute correct predictions by checking element-wise equality
        correct = tf.reduce_sum(tf.cast(tf.reduce_all(tf.equal(y_true_dense, y_pred_dense), axis=-1), dtype=tf.float32))
        total = tf.cast(tf.shape(y_true_dense)[0], dtype=tf.float32)

        # Update the accuracy metric
        self.correct_predictions.assign_add(correct)
        self.total_samples.assign_add(total)

    def result(self):
        return self.correct_predictions / (self.total_samples + K.epsilon())

    def reset_state(self):
        self.correct_predictions.assign(0.0)
        self.total_samples.assign(0.0)
           
# === Configuration ===
serial_port = '/dev/tty.usbmodem101'  # Change as needed (e.g., 'COM3' for Windows)
baud_rate = 9600  # Must match Arduino's baud rate
model_folder = "model_parameters"  # Folder containing trained models (.h5)
model_filename = "cnn_model.h5"  # Change based on the model to use ("cnn_model.h5" or "cldnn_model.h5")
prediction_file_name = "predicted_characters.csv"
max_sequence_length = 27  # Ensure consistency with model training

# === Load Trained Model ===
model_path = os.path.join(model_folder, model_filename)
if os.path.exists(model_path):
    print(f"‚úÖ Loading model from: {model_path}")
    model = load_model(model_path, custom_objects={'ctc_loss_fn': ctc_loss_fn, 'CTCAccuracy': CTCAccuracy})
else:
    print(f"‚ùå ERROR: Model file {model_path} not found!")
    model = None  # Prevent crashes if model is missing

# === Prepare CSV Logging ===
current_directory = os.getcwd()
prediction_path = os.path.join(current_directory, "predicted_data", prediction_file_name)

# Define character set (ensure order matches training data)
noise = ['noise']
english_alphabet_capital = [chr(i) for i in range(65, 91)]  # 'A' to 'Z'
english_alphabet_lower = [chr(i) for i in range(97, 123)]  # 'a' to 'z'
numbers = [str(i) for i in range(10)]  # '0' to '9'
all_characters = noise + english_alphabet_capital + english_alphabet_lower + numbers
char_to_index = {char: idx for idx, char in enumerate(all_characters)}

all_buffer = []  # Buffer to store all data during recording
predicted_characters = {}

# Initialize StandardScaler for consistency with training
scaler = StandardScaler()

# === Open Serial Connection & CSV File ===
try:
    with serial.Serial(serial_port, baud_rate, timeout=1) as ser, open(prediction_path, mode='w', newline='') as prediction_file:
        feature_set = ['Timestamp', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Gyro_X', 'Gyro_Y', 'Gyro_Z', 'Mag_X', 'Mag_Y', 'Mag_Z', 'Force', 'IR_A', 'Letter']
        prediction_writer = csv.writer(prediction_file)
        prediction_writer.writerow(['Timestamp', 'Predicted_Letter'])

        while True:
            try:
                line = ser.readline().decode('utf-8').strip()

                # === Handle Start/Stop Recording ===
                if line == 'System Deactivated':
                    print("üõë Recording stopped.")

                    if model is not None and len(all_buffer) > 0:
                        # Convert buffered data to NumPy array
                        all_data_np = np.array(all_buffer, dtype=np.float32)

                        # Normalize using the same scaler as in training
                        all_data_np = scaler.fit_transform(all_data_np)

                        # Pad the sequence to match model's expected input size
                        all_data_np = pad_sequences([all_data_np], maxlen=max_sequence_length, padding='post', dtype='float32')

                        # Reshape for model (batch_size=1, time_steps, features)
                        all_data_np = all_data_np.reshape(1, all_data_np.shape[1], all_data_np.shape[2])

                        # Make prediction
                        predictions = model.predict(all_data_np)
                        predicted_label_index = np.argmax(predictions)  # Get the predicted class index
                        predicted_character = all_characters[predicted_label_index]

                        print(f"üî† **Predicted Letter: {predicted_character}**")
                        
                        # Get current timestamp
                        now = datetime.now()
                        timestamp = str(now.strftime('%Y-%m-%d %H:%M:%S') + f".{now.microsecond // 1000:03d}")
                        # Store predicted letter for later analysis
                        predicted_characters[timestamp] = predicted_character              
                        prediction_data = [timestamp, predicted_character, all_characters[i]]
                        # Write to CSV
                        prediction_writer.writerow(prediction_data)    
                        
                    all_buffer.clear()  # Reset buffer after prediction

                elif line == 'System Activated':
                    print("‚úÖ System started recording...")
                    all_buffer.clear()

                # === Read All Data ===
                elif len(line.split(',')) == len(feature_set):
                    data = line.split(',')

                    # Store in buffer for prediction
                    all_buffer.append([float(value) for value in data[0:len(feature_set)]])  # Exclude timestamp and label
                    
                    firstLetter = False

            except Exception as e:
                print(f"‚ö†Ô∏è Error predicting data: {e}")
                report_predicted_characters(predicted_characters)
                break

except KeyboardInterrupt:
    print("\nüõë Prediction stopped by user.")
    report_predicted_characters(predicted_characters)
except Exception as e:
    print(f"‚ùå Failed to open serial port or file: {e}")
    report_predicted_characters(predicted_characters)


    