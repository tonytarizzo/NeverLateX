{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpFx+yhpgVyEyFrGb/Gx5Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonytarizzo/NeverLateX/blob/main/Copy_of_STABILO_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "id": "MoesULMv-ihT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1lzhpy4-oC7",
        "outputId": "06f45e01-497e-4e3a-e29d-279512bf5b32"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Attention**\n",
        "\n",
        "❗❗❗❗❗Run the next two cells only if the corresponding files (X_train_merged.npy, X_test_merged.npy, y_train_merged.npy, y_test_merged.npy) have been deleted from /content/drive/My Drive/NeverLaTeX/AML. Otherwise, skip these two and run the next cell.❗❗❗❗❗\n"
      ],
      "metadata": {
        "id": "l28EZzn1WXqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' import os\n",
        "import numpy as np\n",
        "\n",
        "# Path to the main directory in Google Drive\n",
        "base_path = \"/content/drive/My Drive/NeverLaTeX/AML/onhw-chars_2021-06-30\"\n",
        "\n",
        "# Initialize empty lists to hold the data\n",
        "X_train_list, X_test_list, y_train_list, y_test_list = [], [], [], []\n",
        "\n",
        "# Iterate over subdirectories\n",
        "for folder in os.listdir(base_path):\n",
        "    folder_path = os.path.join(base_path, folder)\n",
        "    if os.path.isdir(folder_path):  # Ensure it's a directory\n",
        "        try:\n",
        "            # Load .npy files with allow_pickle=True\n",
        "            X_train = np.load(os.path.join(folder_path, 'X_train.npy'), allow_pickle=True)\n",
        "            X_test = np.load(os.path.join(folder_path, 'X_test.npy'), allow_pickle=True)\n",
        "            y_train = np.load(os.path.join(folder_path, 'y_train.npy'), allow_pickle=True)\n",
        "            y_test = np.load(os.path.join(folder_path, 'y_test.npy'), allow_pickle=True)\n",
        "\n",
        "            # Append to respective lists\n",
        "            X_train_list.append(X_train)\n",
        "            X_test_list.append(X_test)\n",
        "            y_train_list.append(y_train)\n",
        "            y_test_list.append(y_test)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Missing files in {folder_path}, skipping...\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {folder_path}: {e}\")\n",
        "\n",
        "# Concatenate all data\n",
        "X_train_merged = np.concatenate(X_train_list, axis=0)\n",
        "X_test_merged = np.concatenate(X_test_list, axis=0)\n",
        "y_train_merged = np.concatenate(y_train_list, axis=0)\n",
        "y_test_merged = np.concatenate(y_test_list, axis=0)\n",
        "\n",
        "# Print shapes to verify\n",
        "print(f\"X_train_merged shape: {X_train_merged.shape}\")\n",
        "print(f\"X_test_merged shape: {X_test_merged.shape}\")\n",
        "print(f\"y_train_merged shape: {y_train_merged.shape}\")\n",
        "print(f\"y_test_merged shape: {y_test_merged.shape}\") '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A28VBX2BRr9C",
        "outputId": "0056efc5-a4dd-4cd7-80a4-9d566d1b63f5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_merged shape: (466404,)\n",
            "X_test_merged shape: (159096,)\n",
            "y_train_merged shape: (466404,)\n",
            "y_test_merged shape: (159096,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' # Save merged data for future use\n",
        "output_path = '/content/drive/My Drive/NeverLaTeX/AML/onhw-chars_2021-06-30'\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "np.save(os.path.join(output_path, 'X_train_merged.npy'), X_train_merged, allow_pickle=True)\n",
        "np.save(os.path.join(output_path, 'X_test_merged.npy'), X_test_merged, allow_pickle=True)\n",
        "np.save(os.path.join(output_path, 'y_train_merged.npy'), y_train_merged, allow_pickle=True)\n",
        "np.save(os.path.join(output_path, 'y_test_merged.npy'), y_test_merged, allow_pickle=True)\n",
        "\n",
        "print(f\"Merged datasets saved to {output_path}\") '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDWjSddJSFlH",
        "outputId": "963db647-45fb-4f9f-f42e-1e9d9b114874"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged datasets saved to /content/drive/My Drive/NeverLaTeX/AML/onhw-chars_2021-06-30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "js5BJeVGXse1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Attention**\n",
        "\n",
        "❗❗❗❗❗Run the next cell if the corresponding files (X_train_merged.npy, X_test_merged.npy, y_train_merged.npy, y_test_merged.npy) exist in /content/drive/My Drive/NeverLaTeX/AML. Otherwise, skip this cell and run the previous two instead.❗❗❗❗❗"
      ],
      "metadata": {
        "id": "3Fgtvb4JXgDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' import os\n",
        "import numpy as np\n",
        "\n",
        "base_path = \"/content/drive/My Drive/NeverLaTeX/AML/onhw-chars_2021-06-30\"\n",
        "\n",
        "# Load .npy files with allow_pickle=True\n",
        "X_train = np.load(os.path.join(base_path, 'X_train_merged.npy'), allow_pickle=True)\n",
        "X_test = np.load(os.path.join(base_path, 'X_test_merged.npy'), allow_pickle=True)\n",
        "y_train = np.load(os.path.join(base_path, 'y_train_merged.npy'), allow_pickle=True)\n",
        "y_test = np.load(os.path.join(base_path, 'y_test_merged.npy'), allow_pickle=True) '''"
      ],
      "metadata": {
        "id": "2YoOkVC2XRcZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' # Load Subset of Data from Google Drive\n",
        "data_path = \"/content/drive/My Drive/NeverLaTeX/AML/onhw-chars_2021-06-30/onhw2_both_indep_0\"\n",
        "\n",
        "X_train_path = f'{data_path}/X_train.npy'\n",
        "X_test_path = f'{data_path}/X_test.npy'\n",
        "y_train_path = f'{data_path}/y_train.npy'\n",
        "y_test_path = f'{data_path}/y_test.npy' '''"
      ],
      "metadata": {
        "id": "9hFE5xv4_Hfa"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' # Load the data\n",
        "import numpy as np\n",
        "\n",
        "X_train = np.load(X_train_path, allow_pickle=True)\n",
        "X_test = np.load(X_test_path, allow_pickle=True)\n",
        "y_train = np.load(y_train_path, allow_pickle=True)\n",
        "y_test = np.load(y_test_path, allow_pickle=True) '''"
      ],
      "metadata": {
        "id": "HXDpgxNY_45l"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\") '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0-DdDOpACHK",
        "outputId": "64888105-8469-4649-a46b-a31359b52d84"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (466404,)\n",
            "X_test shape: (159096,)\n",
            "y_train shape: (466404,)\n",
            "y_test shape: (159096,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' lengths = [x.shape[0] for x in X_train]\n",
        "\n",
        "print(f\"Min: {min(lengths)}, Max: {max(lengths)}, Median: {np.median(lengths)}\")\n",
        "\n",
        "# Ensure no empty sequences exist\n",
        "non_empty_indices = [i for i, x in enumerate(X_train) if x.shape[0] > 0]\n",
        "X_train = X_train[non_empty_indices]\n",
        "y_train = y_train[non_empty_indices]\n",
        "\n",
        "non_empty_indices = [i for i, x in enumerate(X_test) if x.shape[0] > 0]\n",
        "X_test = X_test[non_empty_indices]\n",
        "y_test = y_test[non_empty_indices]\n",
        "\n",
        "lengths = [x.shape[0] for x in X_train]\n",
        "\n",
        "print(f\"Min: {min(lengths)}, Max: {max(lengths)}, Median: {np.median(lengths)}\") '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj-uTFmeURKu",
        "outputId": "ea5da3fd-3fc5-4d7b-dea6-e96886339044"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min: 0, Max: 3475, Median: 43.0\n",
            "Min: 1, Max: 3475, Median: 43.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' from scipy.signal import resample\n",
        "\n",
        "series_names = [\n",
        "      'acc1x',\n",
        "      'acc1y',\n",
        "      'acc1z',\n",
        "      'acc2x',\n",
        "      'acc2y',\n",
        "      'acc2z',\n",
        "      'gyrox',\n",
        "      'gyroy',\n",
        "      'gyroz',\n",
        "      'magnx',\n",
        "      'magny',\n",
        "      'magnz',\n",
        "      'force',\n",
        "  ]\n",
        "\n",
        "def get_resampled_data(X, y, target_len=43):\n",
        "    data_resampled = []\n",
        "    new_y = []\n",
        "\n",
        "    for item_idx, item in enumerate(X):\n",
        "        if item.shape[0] == 0:  # Skip empty samples\n",
        "            continue\n",
        "\n",
        "        # Resample each feature across timesteps\n",
        "        resampled_item = np.array([resample(item[:, series_idx], target_len) for series_idx in range(len(series_names))])\n",
        "\n",
        "        # Transpose to shape (timesteps, features)\n",
        "        resampled_item = resampled_item.T  # Shape: (target_len, 13)\n",
        "\n",
        "        data_resampled.append(resampled_item)\n",
        "        new_y.append(y[item_idx])\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    data_resampled = np.array(data_resampled)  # Shape: (samples, timesteps, features)\n",
        "    new_y = np.array(new_y)  # Shape: (samples,)\n",
        "\n",
        "    return data_resampled, new_y\n",
        " '''"
      ],
      "metadata": {
        "id": "9tEWzl7uAE5X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' X_train_resampled, y_train_resampled = get_resampled_data(X_train, y_train)\n",
        "X_test_resampled, y_test_resampled = get_resampled_data(X_test, y_test)\n",
        "\n",
        "output_path = '/content/drive/My Drive/NeverLaTeX/AML/onhw-chars_2021-06-30'\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "np.save(os.path.join(output_path, 'X_train_resampled.npy'), X_train_resampled, allow_pickle=True)\n",
        "np.save(os.path.join(output_path, 'X_test_resampled.npy'), X_test_resampled, allow_pickle=True)\n",
        "np.save(os.path.join(output_path, 'y_train_resampled.npy'), y_train_resampled, allow_pickle=True)\n",
        "np.save(os.path.join(output_path, 'y_test_resampled.npy'), y_test_resampled, allow_pickle=True)\n",
        "\n",
        "print(\"Resampled datasets saved successfully.\") '''"
      ],
      "metadata": {
        "id": "R-RVNB1TAFuo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a451c5-b1fd-41f4-e2ab-4f5ebcee6268"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resampled datasets saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "base_path = \"/content/drive/My Drive/NeverLaTeX/AML/onhw-chars_2021-06-30\"\n",
        "\n",
        "# Load .npy files with allow_pickle=True\n",
        "X_train = np.load(os.path.join(base_path, 'X_train_resampled.npy'), allow_pickle=True)\n",
        "X_test = np.load(os.path.join(base_path, 'X_test_resampled.npy'), allow_pickle=True)\n",
        "y_train = np.load(os.path.join(base_path, 'y_train_resampled.npy'), allow_pickle=True)\n",
        "y_test = np.load(os.path.join(base_path, 'y_test_resampled.npy'), allow_pickle=True)"
      ],
      "metadata": {
        "id": "wkVzd6Wudl9m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' X_train = X_train_resampled\n",
        "X_test = X_test_resampled\n",
        "y_train = y_train_resampled\n",
        "y_test = y_test_resampled\n",
        " '''\n",
        "# Define input shape for the model\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])  # (timesteps, features)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"Input shape for models:\", input_shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c74DICPcJ3z5",
        "outputId": "7fcfd50a-07c4-43d0-941e-47d57217c568"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (466357, 43, 13)\n",
            "X_test shape: (159083, 43, 13)\n",
            "Input shape for models: (43, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Initialize label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder on the training and testing labels\n",
        "all_labels = np.concatenate((y_train, y_test))  # Combine train and test labels for consistency\n",
        "label_encoder.fit(all_labels)\n",
        "\n",
        "# Transform the labels\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "print(f\"Classes: {label_encoder.classes_}\")  # Display the mapping of characters to integers\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train_categorical = to_categorical(y_train_encoded, num_classes)\n",
        "y_test_categorical = to_categorical(y_test_encoded, num_classes)\n",
        "\n",
        "print(f\"Shape of y_train: {y_train_categorical.shape}\")\n",
        "print(f\"Shape of y_test: {y_test_categorical.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-HVK8N5AFrw",
        "outputId": "5fdc8215-6f77-4487-a046-4e7e19b5bfcd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R'\n",
            " 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j'\n",
            " 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n",
            "Shape of y_train: (466357, 52)\n",
            "Shape of y_test: (159083, 52)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CNN Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "\n",
        "def build_cnn(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Conv1D(64, kernel_size=3, activation='relu'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "hUDljlEFAFpI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CLDNN Model\n",
        "from tensorflow.keras.layers import LSTM, BatchNormalization\n",
        "\n",
        "def build_cldnn(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        BatchNormalization(),\n",
        "        LSTM(64, return_sequences=True),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "R28qnuB4AFmC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' from re import X\n",
        "# Train and Evaluate Models\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train_categorical.shape)\n",
        "print(y_test_categorical.shape, \"\\n\")\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])  # Assuming (samples, timesteps, features)\n",
        "\n",
        "# Ensure no empty sequences exist\n",
        "non_empty_indices = [i for i, x in enumerate(X_train) if x.size > 0]\n",
        "X_train = X_train[non_empty_indices]\n",
        "y_train_categorical = y_train_categorical[non_empty_indices]\n",
        "\n",
        "non_empty_indices = [i for i, x in enumerate(X_test) if x.size > 0]\n",
        "X_test = X_test[non_empty_indices]\n",
        "y_test_categorical = y_test_categorical[non_empty_indices]\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train_categorical.shape)\n",
        "print(y_test_categorical.shape)\n",
        " '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtQ8aRwyAFit",
        "outputId": "521b4efa-752d-4c3e-bc4f-f1f52e15e05a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(466357, 43, 13)\n",
            "(159083, 43, 13)\n",
            "(466357, 52)\n",
            "(159083, 52) \n",
            "\n",
            "(466357, 43, 13)\n",
            "(159083, 43, 13)\n",
            "(466357, 52)\n",
            "(159083, 52)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = build_cnn(input_shape, num_classes)\n",
        "cldnn_model = build_cldnn(input_shape, num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryI3l5wUAFVa",
        "outputId": "79803794-4ac3-4bbc-b743-20eeec8295ff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train CNN\n",
        "cnn_history = cnn_model.fit(X_train, y_train_categorical, epochs=50, batch_size=64, validation_data=(X_test, y_test_categorical), verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbvOQHDrMRN6",
        "outputId": "c1fa2b43-9102-4b35-b56a-321ecdebe244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m5921/7287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.0191 - loss: 43.4496"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train CLDNN\n",
        "cldnn_history = cldnn_model.fit(X_train, y_train_categorical, epochs=50, batch_size=64, validation_data=(X_test, y_test_categorical), verbose=1)"
      ],
      "metadata": {
        "id": "oTCV8MpuNg_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate both models\n",
        "cnn_loss, cnn_acc = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
        "cldnn_loss, cldnn_acc = cldnn_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"CNN Accuracy: {cnn_acc:.4f}, Loss: {cnn_loss:.4f}\")\n",
        "print(f\"CLDNN Accuracy: {cldnn_acc:.4f}, Loss: {cldnn_loss:.4f}\")"
      ],
      "metadata": {
        "id": "p7lVi0eyNhdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(history, title):\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(cnn_history, 'CNN Accuracy')\n",
        "plot_history(cldnn_history, 'CLDNN Accuracy')"
      ],
      "metadata": {
        "id": "qr_0XcmMZsiN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}